{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 373400 entries, 0 to 373399\n",
      "Data columns (total 50 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   Address                   373400 non-null  object \n",
      " 1   Brain Class               373400 non-null  bool   \n",
      " 2   Data Class                373400 non-null  bool   \n",
      " 3   Futile Abstract Pipeline  373400 non-null  bool   \n",
      " 4   Futile Hierarchy          373400 non-null  bool   \n",
      " 5   God Class                 373400 non-null  bool   \n",
      " 6   Hierarchy Duplication     373400 non-null  bool   \n",
      " 7   Model Class               373400 non-null  bool   \n",
      " 8   Schizofrenic Class        373400 non-null  bool   \n",
      " 9   ABUSEINH                  373400 non-null  int64  \n",
      " 10  AMW                       373400 non-null  float64\n",
      " 11  ATFD                      373400 non-null  int64  \n",
      " 12  BOvM                      373400 non-null  int64  \n",
      " 13  BUR                       373400 non-null  float64\n",
      " 14  CBO                       373400 non-null  int64  \n",
      " 15  CC                        373400 non-null  int64  \n",
      " 16  CM                        373400 non-null  int64  \n",
      " 17  CRIX                      373400 non-null  float64\n",
      " 18  DAC                       373400 non-null  int64  \n",
      " 19  DIT                       373400 non-null  int64  \n",
      " 20  EDUPCLS                   373400 non-null  int64  \n",
      " 21  FANOUT                    373400 non-null  int64  \n",
      " 22  FDP                       373400 non-null  int64  \n",
      " 23  GREEDY                    373400 non-null  int64  \n",
      " 24  HDUPCLS                   373400 non-null  int64  \n",
      " 25  HIT                       373400 non-null  int64  \n",
      " 26  IDUPLINES                 373400 non-null  int64  \n",
      " 27  LOCC                      373400 non-null  int64  \n",
      " 28  NAS                       373400 non-null  int64  \n",
      " 29  NAbsM                     373400 non-null  int64  \n",
      " 30  NDU                       373400 non-null  int64  \n",
      " 31  NOA                       373400 non-null  int64  \n",
      " 32  NOAM                      373400 non-null  int64  \n",
      " 33  NOD                       373400 non-null  int64  \n",
      " 34  NODD                      373400 non-null  int64  \n",
      " 35  NOM                       373400 non-null  int64  \n",
      " 36  NOPA                      373400 non-null  int64  \n",
      " 37  NProtM                    373400 non-null  int64  \n",
      " 38  NSPECM                    373400 non-null  int64  \n",
      " 39  NTempF                    373400 non-null  int64  \n",
      " 40  NrBM                      373400 non-null  int64  \n",
      " 41  NrEC                      373400 non-null  int64  \n",
      " 42  NrFE                      373400 non-null  int64  \n",
      " 43  NrIC                      373400 non-null  int64  \n",
      " 44  NrSS                      373400 non-null  int64  \n",
      " 45  PNAS                      373400 non-null  float64\n",
      " 46  SCHIZO                    373400 non-null  int64  \n",
      " 47  TCC                       373400 non-null  float64\n",
      " 48  WMC                       373400 non-null  int64  \n",
      " 49  WOC                       373400 non-null  float64\n",
      "dtypes: bool(8), float64(6), int64(35), object(1)\n",
      "memory usage: 122.5+ MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#1. LOADING & PRE-PROCESSING CLASS-LEVEL DATASET\n",
    "# Load class-level dataset\n",
    "df = pd.read_csv('Data/class-smell2.csv', low_memory=False)\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing data in dataset\n",
    "for col in df.columns:\n",
    "  missing_data=df[col].isna().sum()\n",
    "  if (missing_data>0):\n",
    "    print(f\"column {col} has {missing_data} missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and initialise a predictive result dataset\n",
    "rs= pd.DataFrame({'Code_smell':[],'Algo':[],'Balance':[],'Ratio':[] , 'Accuracy':[],'Precision':[], 'F1_score':[],'AUC':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BUILDING THE MACHINELEARNING MODEL\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, roc_auc_score, f1_score\n",
    "def _train_and_test(model, _data_train, algo):\n",
    "  global newResult, accuracy,precision,f1,roc\n",
    "  model.fit(_data_train[features], _data_train[target])\n",
    "  predictions = model.predict_proba(data_test[features])\n",
    "  pred_label = model.predict(data_test[features]) \n",
    "  accuracy = accuracy_score(data_test[target], pred_label)\n",
    "  precision = precision_score(data_test[target], pred_label)\n",
    "  f1 = f1_score(data_test[target], pred_label)\n",
    "  roc = roc_auc_score(data_test[target], predictions[:,1])\n",
    "  print('{} Accuracy score on test: {}'.format(algo, accuracy))\n",
    "  print('{} Precision score on test: {}'.format(algo, precision))\n",
    "  print('{} ROC score on test: {}'.format(algo, roc))\n",
    "  print('{} F1 score on test: {}'.format(algo, f1))\n",
    "  print('{} Classification Report: '.format(algo))\n",
    "  print(classification_report(data_test[target], pred_label))\n",
    "  newResult = {'Code_smell':target,'Algo':_algo,'Balance':_balance,'Ratio':_ratio , 'Accuracy':accuracy,'Precision':precision, 'F1_score':f1,'AUC':roc}\n",
    "  return newResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SEQUENTLY, CODE SMELL PREDICTING BY EACH OTHER MODELS\n",
    "features = list(df.select_dtypes(include=['int64', 'float64']).columns)\n",
    "target = 'Brain Class'\n",
    "df[target] = df[target].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Brain-class dataset into subsets: training-set, validation-set, and testing-set.\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "id_pos = np.where(y.values.reshape(-1) == 1)[0]\n",
    "id_neg = np.where(y.values.reshape(-1) == 0)[0]\n",
    "\n",
    "np.random.shuffle(id_pos)\n",
    "np.random.shuffle(id_neg)\n",
    "\n",
    "train_pos_size = 500\n",
    "train_neg_size = 223500\n",
    "val_pos_size = 170\n",
    "val_neg_size = 74500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training-set:\n",
    "id_train_pos = id_pos[:train_pos_size]\n",
    "id_train_neg = id_neg[:train_neg_size] \n",
    "id_train = np.concatenate((id_train_pos, id_train_neg), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating validation-set:\n",
    "id_val_pos = id_pos[train_pos_size:(train_pos_size + val_pos_size)]\n",
    "id_val_neg = id_neg[train_neg_size:(train_neg_size + val_neg_size)]\n",
    "id_val = np.concatenate((id_val_pos, id_val_neg), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating testing-set:\n",
    "id_test_pos = id_pos[(train_pos_size + val_pos_size):(train_pos_size + 2*val_pos_size)]\n",
    "id_test_neg = id_neg[(train_neg_size + val_neg_size):(train_neg_size + 2*val_neg_size)]\n",
    "id_test = np.concatenate((id_test_pos, id_test_neg), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize datasets\n",
    "data_train = df.iloc[id_train]\n",
    "data_val = df.iloc[id_val]\n",
    "data_test = df.iloc[id_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using the Undersampling method, balancing the training-set in different ratios \n",
    "# Create the training-set in the ratio 80:20 (~ 4*train_pos_size:train_pos_size) by keeping 4*train_pos_size random negative samples from it.\n",
    "np.random.shuffle(id_train_neg)\n",
    "id_train_neg_80_20 = id_train_neg[:4*train_pos_size]\n",
    "id_train_80_20 = np.concatenate((id_train_neg_80_20, id_train_pos), axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training-set in the ratio 75:25 (~ 3*train_pos_size:train_pos_size) by keeping 3*train_pos_size random negative samples from it.\n",
    "np.random.shuffle(id_train_neg)\n",
    "id_train_neg_75_25 = id_train_neg[:3*train_pos_size]\n",
    "id_train_75_25 = np.concatenate((id_train_neg_75_25, id_train_pos), axis = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training-set in the ratio 60:40 (~ 1.5*train_pos_size:train_pos_size) by keeping 1.5*train_pos_size random negative samples from it.\n",
    "np.random.shuffle(id_train_neg)\n",
    "id_train_neg_60_40 = id_train_neg[:int(1.5*train_pos_size)]\n",
    "id_train_60_40 = np.concatenate((id_train_neg_60_40, id_train_pos), axis = 0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize training-set\n",
    "data_train_80_20 = df.iloc[id_train_80_20]\n",
    "data_train_75_25 = df.iloc[id_train_75_25]\n",
    "data_train_60_40 = df.iloc[id_train_60_40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 F1 score on val dataset:  0.7813620071684588\n",
      "model 2 F1 score on val dataset:  0.5405405405405406\n",
      "model 3 F1 score on val dataset:  0.5629139072847682\n"
     ]
    }
   ],
   "source": [
    "#The validation-set is used for model tuning to determine the best-selected model.\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_1 = RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=5,\n",
    "                                min_samples_split=200,\n",
    "                                class_weight=None,\n",
    "                                max_features=10)\n",
    "\n",
    "model_2 = RandomForestClassifier(n_estimators=500, \n",
    "                                max_depth=10, \n",
    "                                min_samples_split=400, \n",
    "                                random_state=12, \n",
    "                                class_weight=\"balanced\",\n",
    "                                max_features=\"sqrt\")\n",
    "\n",
    "model_3 = RandomForestClassifier(n_estimators=800, \n",
    "                                max_depth=10, \n",
    "                                min_samples_split=200, \n",
    "                                random_state=12, \n",
    "                                class_weight=\"balanced\",\n",
    "                                max_features=\"sqrt\")\n",
    "\n",
    "def _tunning_model(model , X_train, y_train, X_val, y_val):\n",
    "  model.fit(X_train, y_train)\n",
    "  model_predictions = model.predict_proba(X_val)\n",
    "  model_pred = model.predict(X_val[features]) \n",
    "  model_roc_score = roc_auc_score(y_val, \n",
    "                                  model_predictions[:,1])\n",
    "  model_f1_score = f1_score(y_val, model_pred)\n",
    "  return model, model_roc_score, model_f1_score\n",
    "\n",
    "model_1, model_1_roc_score, model_1_f1_score = _tunning_model(model_1, \n",
    "                                          data_train[features], data_train[target],\n",
    "                                          data_val[features], data_val[target])\n",
    "print('model 1 F1 score on val dataset: ', model_1_f1_score)\n",
    "#print('model 1 ROC score on validation-set: ', model_1_roc_score)\n",
    "\n",
    "model_2, model2_roc_score, model_2_f1_score = _tunning_model(model_2, \n",
    "                                          data_train[features], data_train[target],\n",
    "                                          data_val[features], data_val[target])\n",
    "print('model 2 F1 score on val dataset: ', model_2_f1_score)\n",
    "#print('model 2 ROC score on validation-set: ', model_2_roc_score)\n",
    "\n",
    "\n",
    "model_3, model3_roc_score, model_3_f1_score = _tunning_model(model_3, \n",
    "                                          data_train[features], data_train[target],\n",
    "                                          data_val[features], data_val[target])\n",
    "print('model 3 F1 score on val dataset: ', model_3_f1_score)\n",
    "#print('model 3 ROC score on validation-set: ', model_3_roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3.1 Creating the best-selected model using Random Forest Classifier algorithm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFC_model = RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=5,\n",
    "                                min_samples_split=200,\n",
    "                                class_weight=None,\n",
    "                                max_features=10)\n",
    "_algo = 'RFC'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC_None_* Accuracy score on test: 0.9990491495915361\n",
      "RFC_None_* Precision score on test: 0.9900990099009901\n",
      "RFC_None_* ROC score on test: 0.9999776549545992\n",
      "RFC_None_* F1 score on test: 0.7380073800738007\n",
      "RFC_None_* Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.99      0.59      0.74       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.99      0.79      0.87     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training & testing the model on the imbanlance training-set.\n",
    "_balance ='_None_'\n",
    "_ratio = '*'\n",
    "new_row = pd.DataFrame([_train_and_test(RFC_model, data_train, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC_unsam_80_20 Accuracy score on test: 0.9948573724387304\n",
      "RFC_unsam_80_20 Precision score on test: 0.30685920577617326\n",
      "RFC_unsam_80_20 ROC score on test: 0.9987393604421634\n",
      "RFC_unsam_80_20 F1 score on test: 0.4696132596685083\n",
      "RFC_unsam_80_20 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     74500\n",
      "           1       0.31      1.00      0.47       170\n",
      "\n",
      "    accuracy                           0.99     74670\n",
      "   macro avg       0.65      1.00      0.73     74670\n",
      "weighted avg       1.00      0.99      1.00     74670\n",
      "\n",
      "RFC_unsam_75_25 Accuracy score on test: 0.9949377259943752\n",
      "RFC_unsam_75_25 Precision score on test: 0.3102189781021898\n",
      "RFC_unsam_75_25 ROC score on test: 0.9985388866956177\n",
      "RFC_unsam_75_25 F1 score on test: 0.4735376044568245\n",
      "RFC_unsam_75_25 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     74500\n",
      "           1       0.31      1.00      0.47       170\n",
      "\n",
      "    accuracy                           0.99     74670\n",
      "   macro avg       0.66      1.00      0.74     74670\n",
      "weighted avg       1.00      0.99      1.00     74670\n",
      "\n",
      "RFC_unsam_60_40 Accuracy score on test: 0.9946430962903442\n",
      "RFC_unsam_60_40 Precision score on test: 0.2982456140350877\n",
      "RFC_unsam_60_40 ROC score on test: 0.9978133438610344\n",
      "RFC_unsam_60_40 F1 score on test: 0.4594594594594595\n",
      "RFC_unsam_60_40 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     74500\n",
      "           1       0.30      1.00      0.46       170\n",
      "\n",
      "    accuracy                           0.99     74670\n",
      "   macro avg       0.65      1.00      0.73     74670\n",
      "weighted avg       1.00      0.99      1.00     74670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on training-set with different ratio of Undersampling balancing method.\n",
    "_balance = '_unsam_'\n",
    "\n",
    "# First ratio: 80_20\n",
    "_ratio = '80_20'\n",
    "new_row_80_20 = pd.DataFrame([_train_and_test(RFC_model, data_train_80_20, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_80_20], ignore_index=True)\n",
    "\n",
    "# Second ratio: 75_25\n",
    "_ratio = '75_25'\n",
    "new_row_75_25 = pd.DataFrame([_train_and_test(RFC_model, data_train_75_25, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_75_25], ignore_index=True)\n",
    "\n",
    "# Third ratio: 60_40\n",
    "_ratio = '60_40'\n",
    "new_row_60_40 = pd.DataFrame([_train_and_test(RFC_model, data_train_60_40, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_60_40], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC_oversam_RandomOverSampler Accuracy score on test: 0.995486808624615\n",
      "RFC_oversam_RandomOverSampler Precision score on test: 0.33530571992110453\n",
      "RFC_oversam_RandomOverSampler ROC score on test: 0.9999215159889459\n",
      "RFC_oversam_RandomOverSampler F1 score on test: 0.5022156573116692\n",
      "RFC_oversam_RandomOverSampler Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.34      1.00      0.50       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.67      1.00      0.75     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "RFC_oversam_SMOTE Accuracy score on test: 0.9954198473282443\n",
      "RFC_oversam_SMOTE Precision score on test: 0.33203125\n",
      "RFC_oversam_SMOTE ROC score on test: 0.999206316620608\n",
      "RFC_oversam_SMOTE F1 score on test: 0.49853372434017595\n",
      "RFC_oversam_SMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.33      1.00      0.50       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.67      1.00      0.75     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "RFC_oversam_BorderlineSMOTE Accuracy score on test: 0.9954600241060667\n",
      "RFC_oversam_BorderlineSMOTE Precision score on test: 0.33398821218074654\n",
      "RFC_oversam_BorderlineSMOTE ROC score on test: 0.9992395973154363\n",
      "RFC_oversam_BorderlineSMOTE F1 score on test: 0.5007363770250368\n",
      "RFC_oversam_BorderlineSMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.33      1.00      0.50       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.67      1.00      0.75     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "RFC_oversam_SVMSMOTE Accuracy score on test: 0.9949645105129236\n",
      "RFC_oversam_SVMSMOTE Precision score on test: 0.31135531135531136\n",
      "RFC_oversam_SVMSMOTE ROC score on test: 0.9997547572048953\n",
      "RFC_oversam_SVMSMOTE F1 score on test: 0.4748603351955307\n",
      "RFC_oversam_SVMSMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00     74500\n",
      "           1       0.31      1.00      0.47       170\n",
      "\n",
      "    accuracy                           0.99     74670\n",
      "   macro avg       0.66      1.00      0.74     74670\n",
      "weighted avg       1.00      0.99      1.00     74670\n",
      "\n",
      "RFC_oversam_ADASYN Accuracy score on test: 0.9954332395875184\n",
      "RFC_oversam_ADASYN Precision score on test: 0.33268101761252444\n",
      "RFC_oversam_ADASYN ROC score on test: 0.9991456375838925\n",
      "RFC_oversam_ADASYN F1 score on test: 0.49926578560939794\n",
      "RFC_oversam_ADASYN Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.33      1.00      0.50       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.67      1.00      0.75     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on training-set with different Oversampling balancing method.\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN)\n",
    "\n",
    "oversam = {0: 'RandomOverSampler',\n",
    "           1: 'SMOTE',\n",
    "           2: 'BorderlineSMOTE',\n",
    "           3: 'SVMSMOTE',\n",
    "           4: 'ADASYN'}\n",
    "_balance = '_oversam_'\n",
    "\n",
    "for i, sampler in enumerate((RandomOverSampler(sampling_strategy=1, random_state=0), \n",
    "                             SMOTE(sampling_strategy=1, random_state=0),\n",
    "                             BorderlineSMOTE(sampling_strategy=1, random_state=0, kind='borderline-1'),\n",
    "                             SVMSMOTE(sampling_strategy=1, random_state=0),\n",
    "                             ADASYN(sampling_strategy=1, random_state=0))):\n",
    "    pipe_line = make_pipeline(sampler, RFC_model)\n",
    "    _ratio = oversam[i]\n",
    "    \n",
    "    # Convert the result of _train_and_test to DataFrame\n",
    "    new_row = pd.DataFrame([_train_and_test(pipe_line, data_train, _algo + _balance + _ratio)])\n",
    "    \n",
    "    # Concatenate the new row with the results DataFrame\n",
    "    rs = pd.concat([rs, new_row], ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "rs.to_csv('Class_BrainClass_RFC_rs.csv', header=True, sep=';', decimal=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3.2 Creating the best-selected model using Light Gradient Boosting algorithm\n",
    "import lightgbm as lgb\n",
    "\n",
    "LGB_model = lgb.LGBMClassifier(n_estimator = 800, \n",
    "                                    objective = 'binary', \n",
    "                                    class_weight = 'balanced',\n",
    "                                    learning_rate = 0.05,\n",
    "                                    reg_alpha = 0.1,\n",
    "                                    reg_lambda = 0.1,\n",
    "                                    subsample = 0.8,\n",
    "                                    n_job = -1,\n",
    "                                    random_state = 12\n",
    "                                   )\n",
    "_algo = 'LGB'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 500, number of negative: 223500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3179\n",
      "[LightGBM] [Info] Number of data points in the train set: 224000, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_None_* Accuracy score on test: 0.9999062541850811\n",
      "LGB_None_* Precision score on test: 0.9657142857142857\n",
      "LGB_None_* ROC score on test: 0.9999995262534545\n",
      "LGB_None_* F1 score on test: 0.9797101449275363\n",
      "LGB_None_* Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.97      0.99      0.98       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.98      1.00      0.99     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on the imbalance training-set.\n",
    "_balance = '_None_'\n",
    "_ratio = '*'\n",
    "\n",
    "# Convert the result of _train_and_test to a DataFrame before concatenation\n",
    "new_row = pd.DataFrame([_train_and_test(LGB_model, data_train, _algo + _balance + _ratio)])\n",
    "\n",
    "# Concatenate the new row with the existing DataFrame rs\n",
    "rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 500, number of negative: 2000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1661\n",
      "[LightGBM] [Info] Number of data points in the train set: 2500, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_unsam_80_20 Accuracy score on test: 0.998660774072586\n",
      "LGB_unsam_80_20 Precision score on test: 0.6296296296296297\n",
      "LGB_unsam_80_20 ROC score on test: 0.999997236478484\n",
      "LGB_unsam_80_20 F1 score on test: 0.7727272727272727\n",
      "LGB_unsam_80_20 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.63      1.00      0.77       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.81      1.00      0.89     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 500, number of negative: 1500\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1653\n",
      "[LightGBM] [Info] Number of data points in the train set: 2000, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_unsam_75_25 Accuracy score on test: 0.999812508370162\n",
      "LGB_unsam_75_25 Precision score on test: 0.9239130434782609\n",
      "LGB_unsam_75_25 ROC score on test: 0.9999945124358468\n",
      "LGB_unsam_75_25 F1 score on test: 0.96045197740113\n",
      "LGB_unsam_75_25 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.92      1.00      0.96       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.96      1.00      0.98     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 500, number of negative: 750\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1583\n",
      "[LightGBM] [Info] Number of data points in the train set: 1250, number of used features: 37\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_unsam_60_40 Accuracy score on test: 0.9952591402169546\n",
      "LGB_unsam_60_40 Precision score on test: 0.3244274809160305\n",
      "LGB_unsam_60_40 ROC score on test: 0.9998902092380578\n",
      "LGB_unsam_60_40 F1 score on test: 0.4899135446685879\n",
      "LGB_unsam_60_40 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.32      1.00      0.49       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.66      1.00      0.74     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on training-set with different ratio of Undersampling balancing method.\n",
    "_balance = '_unsam_'\n",
    "\n",
    "# First ratio: 80_20\n",
    "_ratio = '80_20'\n",
    "new_row_80_20 = pd.DataFrame([_train_and_test(LGB_model, data_train_80_20, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_80_20], ignore_index=True)\n",
    "\n",
    "# Second ratio: 75_25\n",
    "_ratio = '75_25'\n",
    "new_row_75_25 = pd.DataFrame([_train_and_test(LGB_model, data_train_75_25, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_75_25], ignore_index=True)\n",
    "\n",
    "# Third ratio: 60_40\n",
    "_ratio = '60_40'\n",
    "new_row_60_40 = pd.DataFrame([_train_and_test(LGB_model, data_train_60_40, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_60_40], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 223500, number of negative: 223500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2804\n",
      "[LightGBM] [Info] Number of data points in the train set: 447000, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_oversam_RandomOverSampler Accuracy score on test: 0.9999732154814517\n",
      "LGB_oversam_RandomOverSampler Precision score on test: 0.9883720930232558\n",
      "LGB_oversam_RandomOverSampler ROC score on test: 0.9999999999999999\n",
      "LGB_oversam_RandomOverSampler F1 score on test: 0.9941520467836257\n",
      "LGB_oversam_RandomOverSampler Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.99      1.00      0.99       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.99      1.00      1.00     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 223500, number of negative: 223500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020470 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5086\n",
      "[LightGBM] [Info] Number of data points in the train set: 447000, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_oversam_SMOTE Accuracy score on test: 0.999812508370162\n",
      "LGB_oversam_SMOTE Precision score on test: 0.9239130434782609\n",
      "LGB_oversam_SMOTE ROC score on test: 0.9999988945913936\n",
      "LGB_oversam_SMOTE F1 score on test: 0.96045197740113\n",
      "LGB_oversam_SMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.92      1.00      0.96       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.96      1.00      0.98     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 223500, number of negative: 223500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5059\n",
      "[LightGBM] [Info] Number of data points in the train set: 447000, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_oversam_BorderlineSMOTE Accuracy score on test: 0.9999196464443552\n",
      "LGB_oversam_BorderlineSMOTE Precision score on test: 0.9659090909090909\n",
      "LGB_oversam_BorderlineSMOTE ROC score on test: 0.9999992104224241\n",
      "LGB_oversam_BorderlineSMOTE F1 score on test: 0.9826589595375722\n",
      "LGB_oversam_BorderlineSMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.97      1.00      0.98       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.98      1.00      0.99     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 223500, number of negative: 223500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5109\n",
      "[LightGBM] [Info] Number of data points in the train set: 447000, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_oversam_SVMSMOTE Accuracy score on test: 0.9997991161108879\n",
      "LGB_oversam_SVMSMOTE Precision score on test: 0.918918918918919\n",
      "LGB_oversam_SVMSMOTE ROC score on test: 0.999993683379392\n",
      "LGB_oversam_SVMSMOTE F1 score on test: 0.9577464788732394\n",
      "LGB_oversam_SVMSMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.92      1.00      0.96       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.96      1.00      0.98     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Info] Number of positive: 223429, number of negative: 223500\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020348 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 5092\n",
      "[LightGBM] [Info] Number of data points in the train set: 446929, number of used features: 38\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Start training from score 0.000000\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "[LightGBM] [Warning] Unknown parameter: n_estimator\n",
      "[LightGBM] [Warning] Unknown parameter: n_job\n",
      "LGB_oversam_ADASYN Accuracy score on test: 0.999812508370162\n",
      "LGB_oversam_ADASYN Precision score on test: 0.9285714285714286\n",
      "LGB_oversam_ADASYN ROC score on test: 0.9999962100276352\n",
      "LGB_oversam_ADASYN F1 score on test: 0.9602272727272727\n",
      "LGB_oversam_ADASYN Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.93      0.99      0.96       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.96      1.00      0.98     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on training-set with different Oversampling balancing method.\n",
    "_balance = '_oversam_'\n",
    "for i, sampler in enumerate((\n",
    "    RandomOverSampler(sampling_strategy=1, random_state=0), \n",
    "    SMOTE(sampling_strategy=1, random_state=0),\n",
    "    BorderlineSMOTE(sampling_strategy=1, random_state=0, kind='borderline-1'),\n",
    "    SVMSMOTE(sampling_strategy=1, random_state=0),\n",
    "    ADASYN(sampling_strategy=1, random_state=0)\n",
    ")):\n",
    "    pipe_line = make_pipeline(sampler, LGB_model)\n",
    "    _ratio = oversam[i]\n",
    "    \n",
    "    # Convert the result of _train_and_test to DataFrame before concatenation\n",
    "    new_row = pd.DataFrame([_train_and_test(pipe_line, data_train, _algo + _balance + _ratio)])\n",
    "    \n",
    "    # Concatenate the new row with the existing DataFrame rs\n",
    "    rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3.3 Creating the best-selected model using KNeighbors Classifier algorithm\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_model = KNeighborsClassifier(n_neighbors = 5, \n",
    "                                      weights = 'distance',\n",
    "                                      algorithm = 'kd_tree',\n",
    "                                      metric = 'minkowski'\n",
    "                                      )\n",
    "_algo = 'KNN'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & testing the model on the imbalance training-set.\n",
    "_balance = '_None_'\n",
    "_ratio = '*'\n",
    "\n",
    "# Convert the result of _train_and_test to DataFrame before concatenation\n",
    "new_row = pd.DataFrame([_train_and_test(KNN_model, data_train, _algo + _balance + _ratio)])\n",
    "\n",
    "# Concatenate the new row with the existing DataFrame rs\n",
    "rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & testing the model on training-set with different ratio of Undersampling balancing method.\n",
    "_balance = '_unsam_'\n",
    "\n",
    "# First ratio: 80_20\n",
    "_ratio = '80_20'\n",
    "new_row_80_20 = pd.DataFrame([_train_and_test(KNN_model, data_train_80_20, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_80_20], ignore_index=True)\n",
    "\n",
    "# Second ratio: 75_25\n",
    "_ratio = '75_25'\n",
    "new_row_75_25 = pd.DataFrame([_train_and_test(KNN_model, data_train_75_25, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_75_25], ignore_index=True)\n",
    "\n",
    "# Third ratio: 60_40\n",
    "_ratio = '60_40'\n",
    "new_row_60_40 = pd.DataFrame([_train_and_test(KNN_model, data_train_60_40, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_60_40], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training & testing the model on training-set with differrent Oversampling balancing method.\n",
    "_balance ='_oversam_'\n",
    "for i, sampler in enumerate((RandomOverSampler(sampling_strategy = 1, random_state=0), \n",
    "                             SMOTE(sampling_strategy = 1, random_state=0),\n",
    "                             BorderlineSMOTE(sampling_strategy = 1, random_state=0, kind='borderline-1'),\n",
    "                             SVMSMOTE(sampling_strategy = 1, random_state=0),\n",
    "                             ADASYN(sampling_strategy = 1, random_state=0))):\n",
    "  pipe_line = make_pipeline(sampler, KNN_model)\n",
    "  _ratio = oversam[i]\n",
    "  rs = rs.append(_train_and_test(pipe_line, data_train, _algo + _balance + _ratio),ignore_index=True)\n",
    "rs.to_csv('Class_BrainClass_KNN_rs.csv', header=True, sep=';', decimal=',') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3.4 Creating the best-selected model using Linear Logistic Regression algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LLR_model = LogisticRegression(C = 0.0001)\n",
    "_algo = 'LLR'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR_None_* Accuracy score on test: 0.9975358242935584\n",
      "LLR_None_* Precision score on test: 0.28125\n",
      "LLR_None_* ROC score on test: 0.4006306356099487\n",
      "LLR_None_* F1 score on test: 0.0891089108910891\n",
      "LLR_None_* Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     74500\n",
      "           1       0.28      0.05      0.09       170\n",
      "\n",
      "    accuracy                           1.00     74670\n",
      "   macro avg       0.64      0.53      0.54     74670\n",
      "weighted avg       1.00      1.00      1.00     74670\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on the imbalance training-set.\n",
    "_balance = '_None_'\n",
    "_ratio = '*'\n",
    "\n",
    "# Convert the result of _train_and_test to DataFrame before concatenation\n",
    "new_row = pd.DataFrame([_train_and_test(LLR_model, data_train, _algo + _balance + _ratio)])\n",
    "\n",
    "# Concatenate the new row with the existing DataFrame rs\n",
    "rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR_unsam_80_20 Accuracy score on test: 0.9770188830855765\n",
      "LLR_unsam_80_20 Precision score on test: 0.08663101604278074\n",
      "LLR_unsam_80_20 ROC score on test: 0.9924127516778524\n",
      "LLR_unsam_80_20 F1 score on test: 0.1588235294117647\n",
      "LLR_unsam_80_20 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     74500\n",
      "           1       0.09      0.95      0.16       170\n",
      "\n",
      "    accuracy                           0.98     74670\n",
      "   macro avg       0.54      0.97      0.57     74670\n",
      "weighted avg       1.00      0.98      0.99     74670\n",
      "\n",
      "LLR_unsam_75_25 Accuracy score on test: 0.9686755055577876\n",
      "LLR_unsam_75_25 Precision score on test: 0.06533066132264528\n",
      "LLR_unsam_75_25 ROC score on test: 0.9765120015791552\n",
      "LLR_unsam_75_25 F1 score on test: 0.12232645403377111\n",
      "LLR_unsam_75_25 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     74500\n",
      "           1       0.07      0.96      0.12       170\n",
      "\n",
      "    accuracy                           0.97     74670\n",
      "   macro avg       0.53      0.96      0.55     74670\n",
      "weighted avg       1.00      0.97      0.98     74670\n",
      "\n",
      "LLR_unsam_60_40 Accuracy score on test: 0.963626623811437\n",
      "LLR_unsam_60_40 Precision score on test: 0.05798611111111111\n",
      "LLR_unsam_60_40 ROC score on test: 0.9920599289380181\n",
      "LLR_unsam_60_40 F1 score on test: 0.10950819672131147\n",
      "LLR_unsam_60_40 Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98     74500\n",
      "           1       0.06      0.98      0.11       170\n",
      "\n",
      "    accuracy                           0.96     74670\n",
      "   macro avg       0.53      0.97      0.55     74670\n",
      "weighted avg       1.00      0.96      0.98     74670\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on training-set with different ratio of Undersampling balancing method.\n",
    "_balance = '_unsam_'\n",
    "\n",
    "# First ratio: 80_20\n",
    "_ratio = '80_20'\n",
    "new_row_80_20 = pd.DataFrame([_train_and_test(LLR_model, data_train_80_20, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_80_20], ignore_index=True)\n",
    "\n",
    "# Second ratio: 75_25\n",
    "_ratio = '75_25'\n",
    "new_row_75_25 = pd.DataFrame([_train_and_test(LLR_model, data_train_75_25, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_75_25], ignore_index=True)\n",
    "\n",
    "# Third ratio: 60_40\n",
    "_ratio = '60_40'\n",
    "new_row_60_40 = pd.DataFrame([_train_and_test(LLR_model, data_train_60_40, _algo + _balance + _ratio)])\n",
    "rs = pd.concat([rs, new_row_60_40], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR_oversam_RandomOverSampler Accuracy score on test: 0.9685147984464979\n",
      "LLR_oversam_RandomOverSampler Precision score on test: 0.06605650616792678\n",
      "LLR_oversam_RandomOverSampler ROC score on test: 0.9804503355704697\n",
      "LLR_oversam_RandomOverSampler F1 score on test: 0.12374207976146105\n",
      "LLR_oversam_RandomOverSampler Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     74500\n",
      "           1       0.07      0.98      0.12       170\n",
      "\n",
      "    accuracy                           0.97     74670\n",
      "   macro avg       0.53      0.97      0.55     74670\n",
      "weighted avg       1.00      0.97      0.98     74670\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR_oversam_SMOTE Accuracy score on test: 0.9726931833400294\n",
      "LLR_oversam_SMOTE Precision score on test: 0.07231121281464531\n",
      "LLR_oversam_SMOTE ROC score on test: 0.9571607185155943\n",
      "LLR_oversam_SMOTE F1 score on test: 0.13418259023354565\n",
      "LLR_oversam_SMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     74500\n",
      "           1       0.07      0.93      0.13       170\n",
      "\n",
      "    accuracy                           0.97     74670\n",
      "   macro avg       0.54      0.95      0.56     74670\n",
      "weighted avg       1.00      0.97      0.98     74670\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLR_oversam_BorderlineSMOTE Accuracy score on test: 0.9780500870496853\n",
      "LLR_oversam_BorderlineSMOTE Precision score on test: 0.0889759373251259\n",
      "LLR_oversam_BorderlineSMOTE ROC score on test: 0.9607372285827083\n",
      "LLR_oversam_BorderlineSMOTE F1 score on test: 0.16249361267245785\n",
      "LLR_oversam_BorderlineSMOTE Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     74500\n",
      "           1       0.09      0.94      0.16       170\n",
      "\n",
      "    accuracy                           0.98     74670\n",
      "   macro avg       0.54      0.96      0.58     74670\n",
      "weighted avg       1.00      0.98      0.99     74670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training & testing the model on training-set with different Oversampling balancing method.\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN)\n",
    "\n",
    "oversam = {0: 'RandomOverSampler',\n",
    "           1: 'SMOTE',\n",
    "           2: 'BorderlineSMOTE',\n",
    "           3: 'SVMSMOTE',\n",
    "           4: 'ADASYN'}\n",
    "\n",
    "_balance = '_oversam_'\n",
    "\n",
    "for i, sampler in enumerate((RandomOverSampler(sampling_strategy=1, random_state=0), \n",
    "                             SMOTE(sampling_strategy=1, random_state=0),\n",
    "                             BorderlineSMOTE(sampling_strategy=1, random_state=0, kind='borderline-1'),\n",
    "                             SVMSMOTE(sampling_strategy=1, random_state=0),\n",
    "                             ADASYN(sampling_strategy=1, random_state=0))):\n",
    "  \n",
    "    pipe_line = make_pipeline(sampler, LLR_model)\n",
    "    _ratio = oversam[i]\n",
    "    \n",
    "    # Create a DataFrame from the results of _train_and_test\n",
    "    new_row = pd.DataFrame([_train_and_test(pipe_line, data_train, _algo + _balance + _ratio)])\n",
    "    \n",
    "    # Concatenate the new row with the existing DataFrame rs\n",
    "    rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#3.5 Creating the best-selected model using Linear Support Vector Classification algorithm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "SVM_model = LinearSVC(penalty='l2', \n",
    "                           loss='squared_hinge',\n",
    "                           tol=0.0001,\n",
    "                           C=0.9,\n",
    "                           dual=False,\n",
    "                           class_weight='balanced',\n",
    "                           max_iter=1000\n",
    "                          )\n",
    "SVM_model = CalibratedClassifierCV(SVM_model) \n",
    "_algo = 'SVM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & testing the model on the imbalance training-set.\n",
    "_balance = '_None_'\n",
    "_ratio = '*'\n",
    "\n",
    "# Convert the result of _train_and_test to DataFrame before concatenation\n",
    "new_row = pd.DataFrame([_train_and_test(SVM_model, data_train, _algo + _balance + _ratio)])\n",
    "\n",
    "# Concatenate the new row with the existing DataFrame rs\n",
    "rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming previous imports and function definitions are already in place\n",
    "\n",
    "# Initialize an empty DataFrame or ensure 'rs' is defined\n",
    "# Example:\n",
    "rs = pd.DataFrame(columns=['Code_smell', 'Algo', 'Balance', 'Ratio', 'Accuracy', 'Precision', 'F1_score', 'AUC'])\n",
    "\n",
    "# Training & testing the model on training-set with different ratios of Undersampling balancing method\n",
    "_balance = '_unsam_'\n",
    "\n",
    "# List of ratios to use for undersampling\n",
    "ratios = ['80_20', '75_25', '60_40']\n",
    "\n",
    "for _ratio in ratios:\n",
    "    # Call _train_and_test for each undersampling ratio\n",
    "    new_row = pd.DataFrame([_train_and_test(SVM_model, eval(f'data_train_{_ratio.replace(\"_\", \"_\")}', ), _algo + _balance + _ratio)])\n",
    "    \n",
    "    # Concatenate the new row with the existing DataFrame rs\n",
    "    rs = pd.concat([rs, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming previous imports and function definitions are already in place\n",
    "\n",
    "# Initialize an empty DataFrame or ensure 'rs' is defined\n",
    "# Example:\n",
    "rs = pd.DataFrame(columns=['Code_smell', 'Algo', 'Balance', 'Ratio', 'Accuracy', 'Precision', 'F1_score', 'AUC'])\n",
    "\n",
    "# Training & testing the model on training-set with different Oversampling balancing methods\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import (RandomOverSampler, SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN)\n",
    "\n",
    "# Dictionary mapping index to oversampling method names\n",
    "oversam = {\n",
    "    0: 'RandomOverSampler',\n",
    "    1: 'SMOTE',\n",
    "    2: 'BorderlineSMOTE',\n",
    "    3: 'SVMSMOTE',\n",
    "    4: 'ADASYN'\n",
    "}\n",
    "\n",
    "_balance = '_oversam_'\n",
    "\n",
    "# Iterate through each oversampling method\n",
    "for i, sampler in enumerate((RandomOverSampler(sampling_strategy=1, random_state=0),\n",
    "                             SMOTE(sampling_strategy=1, random_state=0),\n",
    "                             BorderlineSMOTE(sampling_strategy=1, random_state=0, kind='borderline-1'),\n",
    "                             SVMSMOTE(sampling_strategy=1, random_state=0),\n",
    "                             ADASYN(sampling_strategy=1, random_state=0))):\n",
    "    \n",
    "    # Create a pipeline with the sampler and SVM model\n",
    "    pipe_line = make_pipeline(sampler, SVM_model)\n",
    "    _ratio = oversam[i]\n",
    "\n",
    "    # Call _train_and_test and convert results to DataFrame\n",
    "    new_row = pd.DataFrame([_train_and_test(pipe_line, data_train, _algo + _balance + _ratio)])\n",
    "\n",
    "    # Concatenate the new row with the existing DataFrame rs\n",
    "    rs = pd.concat([rs, new_row], ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "rs.to_csv('Class_BrainClass_SVM_rs.csv', header=True, sep=';', decimal=',', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

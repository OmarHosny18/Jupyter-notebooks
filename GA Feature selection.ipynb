{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of original DataFrame:  (373400, 50)\n",
      "\n",
      "Missing Values:\n",
      " Series([], dtype: int64)\n",
      "\n",
      "Dropping rows where any label is NaN...\n",
      "Rows before dropping: 373400, Rows after dropping: 373400\n",
      "\n",
      "Duplicate Rows Found: 134040\n",
      "Shape after removing duplicate rows:  (239360, 46)\n",
      "\n",
      "Shape of Training Features:  (191488, 41)\n",
      "Shape of Training Labels:  (191488, 5)\n",
      "Shape of Test Features:  (47872, 41)\n",
      "Shape of Test Labels:  (47872, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\deap\\creator.py:185: RuntimeWarning: A class named 'Individual' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\n",
      "0  \t20    \n",
      "1  \t14    \n",
      "2  \t19    \n",
      "3  \t12    \n",
      "4  \t17    \n",
      "5  \t19    \n",
      "6  \t15    \n",
      "7  \t17    \n",
      "8  \t16    \n",
      "9  \t16    \n",
      "10 \t18    \n",
      "\n",
      "Best Feature Subset: [0, 1, 2, 3, 4, 6, 8, 9, 10, 14, 15, 18, 19, 21, 23, 24, 25, 26, 27, 29, 31, 36, 37, 38, 39, 40]\n",
      "Selected Columns: Index(['ABUSEINH', 'AMW', 'ATFD', 'BOvM', 'BUR', 'CC', 'CRIX', 'DAC', 'DIT',\n",
      "       'GREEDY', 'HDUPCLS', 'LOCC', 'NAS', 'NDU', 'NOAM', 'NOD', 'NODD', 'NOM',\n",
      "       'NOPA', 'NSPECM', 'NrBM', 'PNAS', 'SCHIZO', 'TCC', 'WMC', 'WOC'],\n",
      "      dtype='object')\n",
      "\n",
      "--- Final Model Performance on Test Data ---\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "       Brain Class       0.98      1.00      0.99       182\n",
      "        Data Class       1.00      1.00      1.00      3680\n",
      "         God Class       1.00      1.00      1.00       712\n",
      "Schizofrenic Class       1.00      1.00      1.00      3873\n",
      "       Model Class       1.00      1.00      1.00     47795\n",
      "\n",
      "         micro avg       1.00      1.00      1.00     56242\n",
      "         macro avg       1.00      1.00      1.00     56242\n",
      "      weighted avg       1.00      1.00      1.00     56242\n",
      "       samples avg       1.00      1.00      1.00     56242\n",
      "\n",
      "\n",
      "Final Hamming Loss: 0.0001\n",
      "Test Accuracy: 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\Apps\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, hamming_loss, accuracy_score\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# **Load Dataset**\n",
    "df = pd.read_csv('Data/New-Class-smell.csv')\n",
    "print(\"Shape of original DataFrame: \", df.shape)\n",
    "\n",
    "# **Drop Address Column**\n",
    "df = df.drop(columns=['Address','Hierarchy Duplication','Futile Abstract Pipeline','Futile Hierarchy'])\n",
    "\n",
    "# **Check for Missing Values**\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"\\nMissing Values:\\n\", missing_values[missing_values > 0])\n",
    "\n",
    "# Drop rows where any label in smell_columns is NaN\n",
    "smell_columns = [\n",
    "    'Brain Class', 'Data Class', \n",
    "    'God Class', 'Schizofrenic Class', 'Model Class'\n",
    "]\n",
    "\n",
    "print(\"\\nDropping rows where any label is NaN...\")\n",
    "initial_shape = df.shape\n",
    "df = df.dropna(subset=smell_columns)\n",
    "print(f\"Rows before dropping: {initial_shape[0]}, Rows after dropping: {df.shape[0]}\")\n",
    "\n",
    "# **Check for Duplicate Rows**\n",
    "duplicates = df[df.duplicated()]\n",
    "print(f\"\\nDuplicate Rows Found: {duplicates.shape[0]}\")\n",
    "df = df.drop_duplicates()\n",
    "print(\"Shape after removing duplicate rows: \", df.shape)\n",
    "\n",
    "# **Split Dataset**\n",
    "X = df.drop(columns=smell_columns)  # Features\n",
    "y = df[smell_columns]  # Labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"\\nShape of Training Features: \", X_train.shape)\n",
    "print(\"Shape of Training Labels: \", y_train.shape)\n",
    "print(\"Shape of Test Features: \", X_test.shape)\n",
    "print(\"Shape of Test Labels: \", y_test.shape)\n",
    "\n",
    "# **Genetic Algorithm for Feature Selection**\n",
    "\n",
    "# Create a fitness function to evaluate feature subsets\n",
    "def evaluate_feature_subset(individual):\n",
    "    \"\"\"\n",
    "    Evaluates a feature subset using the classification performance on training data.\n",
    "    \"\"\"\n",
    "    selected_features = [index for index, include in enumerate(individual) if include == 1]\n",
    "    if len(selected_features) == 0:  # Avoid empty feature subsets\n",
    "        return float('inf'),\n",
    "    \n",
    "    # Subset the data based on selected features\n",
    "    X_train_subset = X_train.iloc[:, selected_features]\n",
    "    X_test_subset = X_test.iloc[:, selected_features]\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    multi_label_model = MultiOutputClassifier(RandomForestClassifier(class_weight=\"balanced\", random_state=42))\n",
    "    multi_label_model.fit(X_train_subset, y_train)\n",
    "    y_test_pred = multi_label_model.predict(X_test_subset)\n",
    "    \n",
    "    # Use Hamming loss as the fitness metric (lower is better)\n",
    "    return hamming_loss(y_test, y_test_pred),\n",
    "\n",
    "# Setup DEAP framework for GA\n",
    "num_features = X_train.shape[1]\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # Minimize hamming_loss\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", np.random.randint, 0, 2)  # Binary genes: 0 or 1\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=num_features)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)  # Two-point crossover\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)  # Flip bit mutation\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)  # Tournament selection\n",
    "toolbox.register(\"evaluate\", evaluate_feature_subset)\n",
    "\n",
    "# GA Parameters\n",
    "population_size = 20\n",
    "num_generations = 10\n",
    "crossover_probability = 0.8\n",
    "mutation_probability = 0.2\n",
    "\n",
    "# Initialize population\n",
    "population = toolbox.population(n=population_size)\n",
    "\n",
    "# Run GA\n",
    "best_individuals = algorithms.eaSimple(\n",
    "    population,\n",
    "    toolbox,\n",
    "    cxpb=crossover_probability,\n",
    "    mutpb=mutation_probability,\n",
    "    ngen=num_generations,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Get the best feature subset\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "selected_features = [index for index, include in enumerate(best_individual) if include == 1]\n",
    "print(f\"\\nBest Feature Subset: {selected_features}\")\n",
    "# Map indices to column names\n",
    "selected_columns = X_train.columns[selected_features]\n",
    "print(\"Selected Columns:\", selected_columns)\n",
    "\n",
    "\n",
    "# Subset data based on selected features\n",
    "X_train_selected = X_train.iloc[:, selected_features]\n",
    "X_test_selected = X_test.iloc[:, selected_features]\n",
    "\n",
    "# Train and evaluate final model on selected features\n",
    "final_model = MultiOutputClassifier(RandomForestClassifier(class_weight=\"balanced\", random_state=42))\n",
    "final_model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_test_pred = final_model.predict(X_test_selected)\n",
    "print(\"\\n--- Final Model Performance on Test Data ---\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=smell_columns))\n",
    "\n",
    "# Overall Hamming Loss\n",
    "final_hamming_loss = hamming_loss(y_test, y_test_pred)\n",
    "print(f\"\\nFinal Hamming Loss: {final_hamming_loss:.4f}\")\n",
    "\n",
    "# **Accuracy Score**\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
